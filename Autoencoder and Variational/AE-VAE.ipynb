{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fw7NPdkwJtKA"
   },
   "source": [
    "## 0. Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1605468885318,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "ndprszpRrTgi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Adr_TNGsTI"
   },
   "source": [
    "## **1. Data Preparing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dywrS71fIIZf"
   },
   "source": [
    "### 1.1 Divide the scoring data into training set, validation set and test set according to the ratio of 80%, 10%, and 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7766,
     "status": "ok",
     "timestamp": 1605468990884,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "5pPrfSml9i1w"
   },
   "outputs": [],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_csv('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "\n",
    "ratings.sample(frac=1.0)\n",
    "train_set, test_set = train_test_split(ratings,test_size = 0.2)\n",
    "dev_set, test_set = train_test_split(test_set,test_size = 0.5)\n",
    "train_set = np.array(train_set, dtype = 'int')\n",
    "dev_set = np.array(dev_set, dtype = 'int')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7072,
     "status": "ok",
     "timestamp": 1605468889977,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "17SrgRlcIVeW",
    "outputId": "78cc3363-fb33-4b70-9713-18770f4f7847"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7070,
     "status": "ok",
     "timestamp": 1605468889979,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "qvwIHbpLHhuL",
    "outputId": "cf79bc2b-ffb9-42b0-9142-b40db8f777f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800167, 4)\n",
      "(100021, 4)\n",
      "(100021, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(dev_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI0ZtxKwIYJX"
   },
   "source": [
    "### 1.2 Convert the data into a list form ([ [User 1 rating of all movies], [User 2 rating of all movies] ...], and set the ratings of movies that the user has not rated to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12839,
     "status": "ok",
     "timestamp": 1605469006329,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "wG3ke3SbAJJk"
   },
   "outputs": [],
   "source": [
    "nb_users = int(max(max(train_set[:,0]), max(dev_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(train_set[:,1]), max(dev_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "# Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    #Match the user and movie in the corresponding local data. If the user has not evaluated the movie, then mark â€˜0\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        #create The first level list, id_movies is the id of the movie that a user has seen\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        #id_ratingsRate the rate of a movie for a user\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        #First create a list of all 0s, and then replace the rating score of the movie that the user evaluates with 0, then you can mark the movie that the user has not seen as 0\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        #Because movieID starts from 1, and python starts from 0, so if you want rating to match python, -1\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        #Merge the list created above into a list to be extracted by torch\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "    \n",
    "train_set = convert(train_set)\n",
    "dev_set = convert(dev_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNs_PpoSH3OY"
   },
   "source": [
    "### 1.3 Data loading pytorch comes with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3080,
     "status": "ok",
     "timestamp": 1605469037387,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "L8WNeUI3ASnK"
   },
   "outputs": [],
   "source": [
    "train_set = torch.FloatTensor(train_set)\n",
    "train_set = train_set[torch.sum(train_set!=0,axis=1)!=0] # Delete rows with all 0 ratings\n",
    "dev_set = torch.FloatTensor(dev_set)\n",
    "dev_set = dev_set[torch.sum(dev_set!=0,axis=1)!=0] # Delete rows with all 0 ratings\n",
    "test_set = torch.FloatTensor(test_set)\n",
    "test_set = test_set[torch.sum(test_set!=0,axis=1)!=0] # Delete rows with all 0 ratings\n",
    "\n",
    "train_dataset = TensorDataset(train_set)\n",
    "dev_dataset = TensorDataset(dev_set)\n",
    "test_dataset = TensorDataset(test_set)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 16, shuffle = True)\n",
    "dev_loader = DataLoader(dataset = dev_dataset, batch_size = 16, shuffle = False)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tJCUIK0Ikv9"
   },
   "source": [
    "## **2. Auto Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUqUdgN0PC0o"
   },
   "source": [
    "### 2.1 Define the model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1352,
     "status": "ok",
     "timestamp": 1605468943936,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "J9A3zguDIsE3"
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "  \"\"\"\n",
    "  Auto Encoder (AE) was originally used to learn the representation (encoding) of data. It can be broken down into two parts:\n",
    "\n",
    "  encoder: Reduce the dimensionality of the data;\n",
    "  decoder: It converts the encoding back to its original form. Due to the dimensionality reduction, the neural network needs to learn a low-dimensional representation of the input (latent space) in order to be able to reconstruct the input.\n",
    "\n",
    "  They can be used to predict new recommendations. In order to do this, the input and output are both click vectors (usually the input and output of AE are the same), we will use a large dropout after the input layer.\n",
    "  This means that the model will have to reconstruct the click vector, because an element in the input will be lost, so it has to learn to predict the recommended value of a given click vector.\n",
    "  \"\"\"\n",
    "  def __init__(self, nb_movies, device=\"cuda:0\"):\n",
    "    super(AE, self).__init__()\n",
    "    self.nb_movies = nb_movies\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(self.nb_movies, 512),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.9),\n",
    "        nn.Linear(512, 80),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(80, 32),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(32, 80),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(80, 512),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(512, self.nb_movies)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "    return x\n",
    "\n",
    "def loss_func(recon_x, x):\n",
    "  \"\"\"\n",
    "  For a user, he only scored some movies, so when calculating MSE, only consider the movies he has rated, and ignore the movies he has not rated.\n",
    "  The calculation principle of MSE here is, for example, the scoring data of 5 movies by 2 users is: [[1, 2, 0, 3, 4], [2, 3, 0, 0, 1]]\n",
    "  The scoring data after AE reconstruction is: [[1.1, 2.3, 0, 3.3, 4.7], [2.1, 3.2, 0, 0, 1.2]]\n",
    "  Then first calculate the square of the 2 norm of the two scoring data, and then divide by the number of movies that each user has rated to get the MSE of each user, and then use torch.mean to average to get the MSE of each batch\n",
    "  \"\"\"\n",
    "  MSE = torch.mean(torch.norm((x - recon_x), p=2, dim=1, keepdim=False)**2/torch.sum(recon_x!=0,axis=1))\n",
    "  return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3tz3gEIPKON"
   },
   "source": [
    "### 2.2 Define training, validation and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384823,
     "status": "ok",
     "timestamp": 1605470100696,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "WOqhTH7AIssW",
    "outputId": "401a2f39-803e-43c0-f8e2-84fd70f7d5a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Auto Encoder on device: cuda:0 ====================\n",
      "====> Epoch: 1 Training Average loss: 3.8532, Validating Average loss: 1.0675\n",
      "====> Epoch: 2 Training Average loss: 1.0200, Validating Average loss: 1.0205\n",
      "====> Epoch: 3 Training Average loss: 0.9981, Validating Average loss: 1.0166\n",
      "====> Epoch: 4 Training Average loss: 0.9962, Validating Average loss: 1.0152\n",
      "====> Epoch: 5 Training Average loss: 0.9956, Validating Average loss: 1.0189\n",
      "====> Epoch: 6 Training Average loss: 0.9956, Validating Average loss: 1.0162\n",
      "====> Epoch: 7 Training Average loss: 0.9954, Validating Average loss: 1.0164\n",
      "====> Epoch: 8 Training Average loss: 0.9947, Validating Average loss: 1.0198\n",
      "====> Epoch: 9 Training Average loss: 0.9941, Validating Average loss: 1.0193\n",
      "====> Epoch: 10 Training Average loss: 0.9939, Validating Average loss: 1.0168\n",
      "====> Epoch: 11 Training Average loss: 0.9924, Validating Average loss: 1.0214\n",
      "====> Epoch: 12 Training Average loss: 0.9915, Validating Average loss: 1.0177\n",
      "====> Epoch: 13 Training Average loss: 0.9894, Validating Average loss: 1.0170\n",
      "====> Epoch: 14 Training Average loss: 0.9881, Validating Average loss: 1.0197\n",
      "====> Epoch: 15 Training Average loss: 0.9873, Validating Average loss: 1.0160\n",
      "====> Epoch: 16 Training Average loss: 0.9846, Validating Average loss: 1.0148\n",
      "====> Epoch: 17 Training Average loss: 0.9831, Validating Average loss: 1.0170\n",
      "====> Epoch: 18 Training Average loss: 0.9819, Validating Average loss: 1.0178\n",
      "====> Epoch: 19 Training Average loss: 0.9789, Validating Average loss: 1.0172\n",
      "====> Epoch: 20 Training Average loss: 0.9780, Validating Average loss: 1.0170\n",
      "====> Epoch: 21 Training Average loss: 0.9760, Validating Average loss: 1.0169\n",
      "====> Epoch: 22 Training Average loss: 0.9763, Validating Average loss: 1.0193\n",
      "====> Epoch: 23 Training Average loss: 0.9732, Validating Average loss: 1.0177\n",
      "====> Epoch: 24 Training Average loss: 0.9729, Validating Average loss: 1.0229\n",
      "====> Epoch: 25 Training Average loss: 0.9710, Validating Average loss: 1.0166\n",
      "====> Epoch: 26 Training Average loss: 0.9683, Validating Average loss: 1.0230\n",
      "====> Epoch: 27 Training Average loss: 0.9681, Validating Average loss: 1.0217\n",
      "====> Epoch: 28 Training Average loss: 0.9654, Validating Average loss: 1.0249\n",
      "====> Epoch: 29 Training Average loss: 0.9654, Validating Average loss: 1.0244\n",
      "====> Epoch: 30 Training Average loss: 0.9639, Validating Average loss: 1.0239\n",
      "====> Epoch: 31 Training Average loss: 0.9632, Validating Average loss: 1.0189\n",
      "====> Epoch: 32 Training Average loss: 0.9617, Validating Average loss: 1.0246\n",
      "====> Epoch: 33 Training Average loss: 0.9597, Validating Average loss: 1.0215\n",
      "====> Epoch: 34 Training Average loss: 0.9585, Validating Average loss: 1.0258\n",
      "====> Epoch: 35 Training Average loss: 0.9597, Validating Average loss: 1.0247\n",
      "====> Epoch: 36 Training Average loss: 0.9568, Validating Average loss: 1.0246\n",
      "====> Epoch: 37 Training Average loss: 0.9563, Validating Average loss: 1.0242\n",
      "====> Epoch: 38 Training Average loss: 0.9533, Validating Average loss: 1.0316\n",
      "====> Epoch: 39 Training Average loss: 0.9535, Validating Average loss: 1.0300\n",
      "====> Epoch: 40 Training Average loss: 0.9514, Validating Average loss: 1.0266\n",
      "====> Epoch: 41 Training Average loss: 0.9503, Validating Average loss: 1.0319\n",
      "====> Epoch: 42 Training Average loss: 0.9489, Validating Average loss: 1.0274\n",
      "====> Epoch: 43 Training Average loss: 0.9471, Validating Average loss: 1.0313\n",
      "====> Epoch: 44 Training Average loss: 0.9452, Validating Average loss: 1.0250\n",
      "====> Epoch: 45 Training Average loss: 0.9431, Validating Average loss: 1.0314\n",
      "====> Epoch: 46 Training Average loss: 0.9423, Validating Average loss: 1.0407\n",
      "====> Epoch: 47 Training Average loss: 0.9435, Validating Average loss: 1.0336\n",
      "====> Epoch: 48 Training Average loss: 0.9393, Validating Average loss: 1.0367\n",
      "====> Epoch: 49 Training Average loss: 0.9389, Validating Average loss: 1.0353\n",
      "====> Epoch: 50 Training Average loss: 0.9387, Validating Average loss: 1.0351\n",
      "====> Epoch: 51 Training Average loss: 0.9342, Validating Average loss: 1.0439\n",
      "====> Epoch: 52 Training Average loss: 0.9356, Validating Average loss: 1.0357\n",
      "====> Epoch: 53 Training Average loss: 0.9327, Validating Average loss: 1.0429\n",
      "====> Epoch: 54 Training Average loss: 0.9310, Validating Average loss: 1.0362\n",
      "====> Epoch: 55 Training Average loss: 0.9287, Validating Average loss: 1.0400\n",
      "====> Epoch: 56 Training Average loss: 0.9295, Validating Average loss: 1.0376\n",
      "====> Epoch: 57 Training Average loss: 0.9275, Validating Average loss: 1.0357\n",
      "====> Epoch: 58 Training Average loss: 0.9264, Validating Average loss: 1.0380\n",
      "====> Epoch: 59 Training Average loss: 0.9265, Validating Average loss: 1.0322\n",
      "====> Epoch: 60 Training Average loss: 0.9238, Validating Average loss: 1.0384\n",
      "====> Epoch: 61 Training Average loss: 0.9225, Validating Average loss: 1.0399\n",
      "====> Epoch: 62 Training Average loss: 0.9228, Validating Average loss: 1.0365\n",
      "====> Epoch: 63 Training Average loss: 0.9205, Validating Average loss: 1.0372\n",
      "====> Epoch: 64 Training Average loss: 0.9201, Validating Average loss: 1.0404\n",
      "====> Epoch: 65 Training Average loss: 0.9194, Validating Average loss: 1.0416\n",
      "====> Epoch: 66 Training Average loss: 0.9184, Validating Average loss: 1.0345\n",
      "====> Epoch: 67 Training Average loss: 0.9151, Validating Average loss: 1.0446\n",
      "====> Epoch: 68 Training Average loss: 0.9150, Validating Average loss: 1.0447\n",
      "====> Epoch: 69 Training Average loss: 0.9156, Validating Average loss: 1.0335\n",
      "====> Epoch: 70 Training Average loss: 0.9126, Validating Average loss: 1.0401\n",
      "====> Epoch: 71 Training Average loss: 0.9134, Validating Average loss: 1.0423\n",
      "====> Epoch: 72 Training Average loss: 0.9133, Validating Average loss: 1.0372\n",
      "====> Epoch: 73 Training Average loss: 0.9115, Validating Average loss: 1.0381\n",
      "====> Epoch: 74 Training Average loss: 0.9097, Validating Average loss: 1.0370\n",
      "====> Epoch: 75 Training Average loss: 0.9104, Validating Average loss: 1.0395\n",
      "====> Epoch: 76 Training Average loss: 0.9082, Validating Average loss: 1.0390\n",
      "====> Epoch: 77 Training Average loss: 0.9077, Validating Average loss: 1.0356\n",
      "====> Epoch: 78 Training Average loss: 0.9063, Validating Average loss: 1.0411\n",
      "====> Epoch: 79 Training Average loss: 0.9059, Validating Average loss: 1.0376\n",
      "====> Epoch: 80 Training Average loss: 0.9037, Validating Average loss: 1.0408\n",
      "====> Epoch: 81 Training Average loss: 0.9053, Validating Average loss: 1.0358\n",
      "====> Epoch: 82 Training Average loss: 0.9028, Validating Average loss: 1.0417\n",
      "====> Epoch: 83 Training Average loss: 0.9038, Validating Average loss: 1.0419\n",
      "====> Epoch: 84 Training Average loss: 0.9021, Validating Average loss: 1.0358\n",
      "====> Epoch: 85 Training Average loss: 0.9010, Validating Average loss: 1.0395\n",
      "====> Epoch: 86 Training Average loss: 0.9006, Validating Average loss: 1.0433\n",
      "====> Epoch: 87 Training Average loss: 0.8983, Validating Average loss: 1.0411\n",
      "====> Epoch: 88 Training Average loss: 0.9010, Validating Average loss: 1.0461\n",
      "====> Epoch: 89 Training Average loss: 0.8975, Validating Average loss: 1.0434\n",
      "====> Epoch: 90 Training Average loss: 0.8975, Validating Average loss: 1.0376\n",
      "====> Epoch: 91 Training Average loss: 0.8967, Validating Average loss: 1.0395\n",
      "====> Epoch: 92 Training Average loss: 0.8964, Validating Average loss: 1.0374\n",
      "====> Epoch: 93 Training Average loss: 0.8944, Validating Average loss: 1.0394\n",
      "====> Epoch: 94 Training Average loss: 0.8965, Validating Average loss: 1.0369\n",
      "====> Epoch: 95 Training Average loss: 0.8941, Validating Average loss: 1.0399\n",
      "====> Epoch: 96 Training Average loss: 0.8948, Validating Average loss: 1.0444\n",
      "====> Epoch: 97 Training Average loss: 0.8932, Validating Average loss: 1.0418\n",
      "====> Epoch: 98 Training Average loss: 0.8919, Validating Average loss: 1.0456\n",
      "====> Epoch: 99 Training Average loss: 0.8939, Validating Average loss: 1.0406\n",
      "====> Epoch: 100 Training Average loss: 0.8920, Validating Average loss: 1.0385\n",
      "====> Epoch: 101 Training Average loss: 0.8908, Validating Average loss: 1.0438\n",
      "====> Epoch: 102 Training Average loss: 0.8903, Validating Average loss: 1.0436\n",
      "====> Epoch: 103 Training Average loss: 0.8901, Validating Average loss: 1.0411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 104 Training Average loss: 0.8879, Validating Average loss: 1.0470\n",
      "====> Epoch: 105 Training Average loss: 0.8884, Validating Average loss: 1.0391\n",
      "====> Epoch: 106 Training Average loss: 0.8852, Validating Average loss: 1.0436\n",
      "====> Epoch: 107 Training Average loss: 0.8877, Validating Average loss: 1.0462\n",
      "====> Epoch: 108 Training Average loss: 0.8856, Validating Average loss: 1.0462\n",
      "====> Epoch: 109 Training Average loss: 0.8851, Validating Average loss: 1.0384\n",
      "====> Epoch: 110 Training Average loss: 0.8856, Validating Average loss: 1.0424\n",
      "====> Epoch: 111 Training Average loss: 0.8846, Validating Average loss: 1.0456\n",
      "====> Epoch: 112 Training Average loss: 0.8847, Validating Average loss: 1.0387\n",
      "====> Epoch: 113 Training Average loss: 0.8846, Validating Average loss: 1.0385\n",
      "====> Epoch: 114 Training Average loss: 0.8821, Validating Average loss: 1.0459\n",
      "====> Epoch: 115 Training Average loss: 0.8791, Validating Average loss: 1.0414\n",
      "====> Epoch: 116 Training Average loss: 0.8809, Validating Average loss: 1.0458\n",
      "====> Epoch: 117 Training Average loss: 0.8792, Validating Average loss: 1.0519\n",
      "====> Epoch: 118 Training Average loss: 0.8805, Validating Average loss: 1.0465\n",
      "====> Epoch: 119 Training Average loss: 0.8783, Validating Average loss: 1.0460\n",
      "====> Epoch: 120 Training Average loss: 0.8772, Validating Average loss: 1.0424\n",
      "====> Epoch: 121 Training Average loss: 0.8768, Validating Average loss: 1.0492\n",
      "====> Epoch: 122 Training Average loss: 0.8740, Validating Average loss: 1.0466\n",
      "====> Epoch: 123 Training Average loss: 0.8745, Validating Average loss: 1.0472\n",
      "====> Epoch: 124 Training Average loss: 0.8744, Validating Average loss: 1.0448\n",
      "====> Epoch: 125 Training Average loss: 0.8725, Validating Average loss: 1.0509\n",
      "====> Epoch: 126 Training Average loss: 0.8699, Validating Average loss: 1.0521\n",
      "====> Epoch: 127 Training Average loss: 0.8699, Validating Average loss: 1.0428\n",
      "====> Epoch: 128 Training Average loss: 0.8676, Validating Average loss: 1.0494\n",
      "====> Epoch: 129 Training Average loss: 0.8672, Validating Average loss: 1.0478\n",
      "====> Epoch: 130 Training Average loss: 0.8653, Validating Average loss: 1.0464\n",
      "====> Epoch: 131 Training Average loss: 0.8654, Validating Average loss: 1.0492\n",
      "====> Epoch: 132 Training Average loss: 0.8633, Validating Average loss: 1.0514\n",
      "====> Epoch: 133 Training Average loss: 0.8621, Validating Average loss: 1.0443\n",
      "====> Epoch: 134 Training Average loss: 0.8631, Validating Average loss: 1.0507\n",
      "====> Epoch: 135 Training Average loss: 0.8612, Validating Average loss: 1.0542\n",
      "====> Epoch: 136 Training Average loss: 0.8604, Validating Average loss: 1.0510\n",
      "====> Epoch: 137 Training Average loss: 0.8586, Validating Average loss: 1.0555\n",
      "====> Epoch: 138 Training Average loss: 0.8579, Validating Average loss: 1.0527\n",
      "====> Epoch: 139 Training Average loss: 0.8583, Validating Average loss: 1.0507\n",
      "====> Epoch: 140 Training Average loss: 0.8550, Validating Average loss: 1.0559\n",
      "====> Epoch: 141 Training Average loss: 0.8545, Validating Average loss: 1.0452\n",
      "====> Epoch: 142 Training Average loss: 0.8547, Validating Average loss: 1.0469\n",
      "====> Epoch: 143 Training Average loss: 0.8522, Validating Average loss: 1.0587\n",
      "====> Epoch: 144 Training Average loss: 0.8543, Validating Average loss: 1.0525\n",
      "====> Epoch: 145 Training Average loss: 0.8522, Validating Average loss: 1.0498\n",
      "====> Epoch: 146 Training Average loss: 0.8505, Validating Average loss: 1.0462\n",
      "====> Epoch: 147 Training Average loss: 0.8514, Validating Average loss: 1.0476\n",
      "====> Epoch: 148 Training Average loss: 0.8482, Validating Average loss: 1.0567\n",
      "====> Epoch: 149 Training Average loss: 0.8472, Validating Average loss: 1.0593\n",
      "====> Epoch: 150 Training Average loss: 0.8475, Validating Average loss: 1.0504\n",
      "====> Epoch: 151 Training Average loss: 0.8464, Validating Average loss: 1.0563\n",
      "====> Epoch: 152 Training Average loss: 0.8463, Validating Average loss: 1.0491\n",
      "====> Epoch: 153 Training Average loss: 0.8469, Validating Average loss: 1.0517\n",
      "====> Epoch: 154 Training Average loss: 0.8429, Validating Average loss: 1.0540\n",
      "====> Epoch: 155 Training Average loss: 0.8423, Validating Average loss: 1.0506\n",
      "====> Epoch: 156 Training Average loss: 0.8432, Validating Average loss: 1.0483\n",
      "====> Epoch: 157 Training Average loss: 0.8438, Validating Average loss: 1.0525\n",
      "====> Epoch: 158 Training Average loss: 0.8417, Validating Average loss: 1.0572\n",
      "====> Epoch: 159 Training Average loss: 0.8394, Validating Average loss: 1.0519\n",
      "====> Epoch: 160 Training Average loss: 0.8399, Validating Average loss: 1.0479\n",
      "====> Epoch: 161 Training Average loss: 0.8385, Validating Average loss: 1.0438\n",
      "====> Epoch: 162 Training Average loss: 0.8400, Validating Average loss: 1.0404\n",
      "====> Epoch: 163 Training Average loss: 0.8378, Validating Average loss: 1.0500\n",
      "====> Epoch: 164 Training Average loss: 0.8362, Validating Average loss: 1.0509\n",
      "====> Epoch: 165 Training Average loss: 0.8381, Validating Average loss: 1.0433\n",
      "====> Epoch: 166 Training Average loss: 0.8371, Validating Average loss: 1.0478\n",
      "====> Epoch: 167 Training Average loss: 0.8357, Validating Average loss: 1.0418\n",
      "====> Epoch: 168 Training Average loss: 0.8352, Validating Average loss: 1.0570\n",
      "====> Epoch: 169 Training Average loss: 0.8327, Validating Average loss: 1.0557\n",
      "====> Epoch: 170 Training Average loss: 0.8327, Validating Average loss: 1.0512\n",
      "====> Epoch: 171 Training Average loss: 0.8317, Validating Average loss: 1.0532\n",
      "====> Epoch: 172 Training Average loss: 0.8317, Validating Average loss: 1.0444\n",
      "====> Epoch: 173 Training Average loss: 0.8325, Validating Average loss: 1.0560\n",
      "====> Epoch: 174 Training Average loss: 0.8305, Validating Average loss: 1.0526\n",
      "====> Epoch: 175 Training Average loss: 0.8312, Validating Average loss: 1.0569\n",
      "====> Epoch: 176 Training Average loss: 0.8288, Validating Average loss: 1.0623\n",
      "====> Epoch: 177 Training Average loss: 0.8284, Validating Average loss: 1.0652\n",
      "====> Epoch: 178 Training Average loss: 0.8289, Validating Average loss: 1.0663\n",
      "====> Epoch: 179 Training Average loss: 0.8290, Validating Average loss: 1.0577\n",
      "====> Epoch: 180 Training Average loss: 0.8272, Validating Average loss: 1.0551\n",
      "====> Epoch: 181 Training Average loss: 0.8258, Validating Average loss: 1.0591\n",
      "====> Epoch: 182 Training Average loss: 0.8257, Validating Average loss: 1.0624\n",
      "====> Epoch: 183 Training Average loss: 0.8247, Validating Average loss: 1.0603\n",
      "====> Epoch: 184 Training Average loss: 0.8241, Validating Average loss: 1.0560\n",
      "====> Epoch: 185 Training Average loss: 0.8230, Validating Average loss: 1.0576\n",
      "====> Epoch: 186 Training Average loss: 0.8218, Validating Average loss: 1.0585\n",
      "====> Epoch: 187 Training Average loss: 0.8222, Validating Average loss: 1.0513\n",
      "====> Epoch: 188 Training Average loss: 0.8219, Validating Average loss: 1.0674\n",
      "====> Epoch: 189 Training Average loss: 0.8213, Validating Average loss: 1.0712\n",
      "====> Epoch: 190 Training Average loss: 0.8196, Validating Average loss: 1.0624\n",
      "====> Epoch: 191 Training Average loss: 0.8217, Validating Average loss: 1.0551\n",
      "====> Epoch: 192 Training Average loss: 0.8203, Validating Average loss: 1.0620\n",
      "====> Epoch: 193 Training Average loss: 0.8199, Validating Average loss: 1.0561\n",
      "====> Epoch: 194 Training Average loss: 0.8213, Validating Average loss: 1.0579\n",
      "====> Epoch: 195 Training Average loss: 0.8199, Validating Average loss: 1.0677\n",
      "====> Epoch: 196 Training Average loss: 0.8157, Validating Average loss: 1.0559\n",
      "====> Epoch: 197 Training Average loss: 0.8155, Validating Average loss: 1.0667\n",
      "====> Epoch: 198 Training Average loss: 0.8170, Validating Average loss: 1.0785\n",
      "====> Epoch: 199 Training Average loss: 0.8157, Validating Average loss: 1.0796\n",
      "====> Epoch: 200 Training Average loss: 0.8163, Validating Average loss: 1.0675\n",
      "\n",
      " ==================== Testing Auto Encoder on device: cuda:0 ====================\n",
      "Average test loss: 1.0503\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, dev_loader=None, is_validate=True, device=\"cuda:0\"):\n",
    "  ae.train()\n",
    "  total_loss = 0\n",
    "  for _, data in enumerate(train_loader, 0):\n",
    "    data = Variable(data[0]).to(device)\n",
    "    target = data.clone()\n",
    "    optimizer.zero_grad()\n",
    "    recon_x = ae.forward(data)\n",
    "    # In the optimization process, we only want to consider the movies that users have rated,\n",
    "    # Although we previously set the ratings of unrated movies by users to 0, we also need to set the ratings of unrated movies predicted by the model to 0.\n",
    "    # This will not accumulate to the loss, which will affect the weight update\n",
    "    recon_x[target == 0] = 0 \n",
    "    loss = loss_func(recon_x, data)\n",
    "    loss.backward()\n",
    "    total_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    epoch_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "  if (is_validate == True):\n",
    "    ae.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for _, data in enumerate(dev_loader, 0):\n",
    "        data = Variable(data[0]).to(device)\n",
    "        target = data.clone()\n",
    "        recon_x = ae.forward(data)\n",
    "        recon_x[target == 0] = 0\n",
    "        loss = loss_func(recon_x, data)\n",
    "        total_loss += loss.item()\n",
    "        epoch_dev_loss = total_loss / len(dev_loader)\n",
    "    print('====> Epoch: {} Training Average loss: {:.4f}, Validating Average loss: {:.4f}'.format(epoch, epoch_train_loss, epoch_dev_loss))\n",
    "    return epoch_train_loss, epoch_dev_loss\n",
    "\n",
    "  else:\n",
    "    print('====> Epoch: {} Training Average loss: {:.4f}'.format(epoch, epoch_train_loss))\n",
    "    return epoch_train_loss\n",
    "\n",
    "def test(test_loader, device=\"cuda:0\"):\n",
    "  ae.eval()\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for _, data in enumerate(test_loader, 0):\n",
    "      data = Variable(data[0]).to(device)\n",
    "      target = data.clone()\n",
    "      recon_x = ae.forward(data)\n",
    "      recon_x[target == 0] = 0\n",
    "      loss = loss_func(recon_x, data)\n",
    "      total_loss += loss.item()\n",
    "  print('Average test loss: {:.4f}'.format(total_loss / len(test_loader)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  ae = AE(nb_movies = nb_movies).to(device)\n",
    "  optimizer =  optim.Adam(ae.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "  EPOCH = 200\n",
    "  epoches_train_loss = []\n",
    "  epoches_dev_loss = []\n",
    "\n",
    "  print(\"\\n\", 20 * \"=\", \"Training Auto Encoder on device: {}\".format(device), 20 * \"=\")\n",
    "  for epoch in range(1, EPOCH + 1):\n",
    "    epoch_train_loss, epoch_dev_loss = train(train_loader, dev_loader = dev_loader)\n",
    "    epoches_train_loss.append(epoch_train_loss)\n",
    "    epoches_dev_loss.append(epoch_dev_loss)\n",
    "  print(\"\\n\", 20 * \"=\", \"Testing Auto Encoder on device: {}\".format(device), 20 * \"=\")\n",
    "  test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXU81_oWRz3S"
   },
   "source": [
    "### 2.3 Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "87X-Lh5RRwNz"
   },
   "outputs": [],
   "source": [
    "# save the training model\n",
    "torch.save(ae.state_dict(), 'AutoEncoder.pkl')\n",
    "\n",
    "# load the trained model\n",
    "ae = torch.load('AutoEncoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-6IRUWzP0Wd"
   },
   "source": [
    "### 2.4 Plot a graph to show the loss change in the training and verification process: It can be found that the loss of the verification set has dropped first and then rises slightly. It is considered that the model is over-fitting + poor generalization ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1605470312644,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "SORypM92IsnS",
    "outputId": "a146157e-ecfa-46eb-ba2a-29d5528ee693"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(np.arange(len(epoches_train_loss)), epoches_train_loss, color='r', label='training loss')\n",
    "plt.plot(np.arange(len(epoches_dev_loss)), epoches_dev_loss, color='b', label='validating loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('MSE-loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4IrkY3Adh3G"
   },
   "source": [
    "## **3. Variational Auto Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy34192Vd1Al"
   },
   "source": [
    "### 3.1 Define the model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1605470953012,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "FvEj4FiKUU2g"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "  def __init__(self, nb_movies, device=\"cuda:0\"):\n",
    "    super(VAE, self).__init__()\n",
    "    self.nb_movies = nb_movies\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(self.nb_movies, 512),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.9), # Need a larger dropout\n",
    "        nn.Linear(512, 80),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "    self.fc1 = nn.Linear(80, 32)\n",
    "    self.fc2 = nn.Linear(80, 32)\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(32, 80),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(80, 512),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(512, self.nb_movies)\n",
    "        )\n",
    "    \n",
    "  # reparameterize\n",
    "  def reparameterize(self, mu, logvar):\n",
    "    eps = Variable(torch.randn(mu.size(0), mu.size(1))).to(device)\n",
    "    z = mu + eps * torch.exp(logvar/2) \n",
    "    return z\n",
    "\n",
    "  def forward(self, x):\n",
    "    out1, out2 = self.encoder(x), self.encoder(x)\n",
    "    mu = self.fc1(out1)\n",
    "    logvar = self.fc2(out2)\n",
    "    z = self.reparameterize(mu, logvar)\n",
    "    return self.decoder(z), mu, logvar\n",
    "\n",
    "def loss_func(recon_x, x, mu, logvar):\n",
    "  \"\"\"\n",
    "  The loss of VAE includes two parts:\n",
    "  One part is the average absolute error between the predicted result and the true result;\n",
    "  The other part is KL-divergence (KL divergence), which is used to measure the difference between the distribution of latent variables and the unit Gaussian distribution.\n",
    "  \"\"\"\n",
    "  MSE = torch.mean(torch.norm((x - recon_x), p=2, dim=1, keepdim=False)**2/torch.sum(recon_x!=0,axis=1))\n",
    "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return MSE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUffuiIkd6DN"
   },
   "source": [
    "### 3.2 Define training, validation and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529746,
     "status": "ok",
     "timestamp": 1605471486931,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "bPHi0HL2AVMK",
    "outputId": "42dcce6b-e802-41e5-905c-cea4bf13bf38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==================== Training Variational Auto Encoder on device: cuda:0 ====================\n",
      "====> Epoch: 1 Training Average loss: 7.8470, Validating Average loss: 1.5987\n",
      "====> Epoch: 2 Training Average loss: 2.1356, Validating Average loss: 1.3609\n",
      "====> Epoch: 3 Training Average loss: 1.6246, Validating Average loss: 1.1745\n",
      "====> Epoch: 4 Training Average loss: 1.3480, Validating Average loss: 1.0744\n",
      "====> Epoch: 5 Training Average loss: 1.1938, Validating Average loss: 1.0363\n",
      "====> Epoch: 6 Training Average loss: 1.1162, Validating Average loss: 1.0301\n",
      "====> Epoch: 7 Training Average loss: 1.0769, Validating Average loss: 1.0296\n",
      "====> Epoch: 8 Training Average loss: 1.0543, Validating Average loss: 1.0327\n",
      "====> Epoch: 9 Training Average loss: 1.0384, Validating Average loss: 1.0309\n",
      "====> Epoch: 10 Training Average loss: 1.0283, Validating Average loss: 1.0271\n",
      "====> Epoch: 11 Training Average loss: 1.0199, Validating Average loss: 1.0290\n",
      "====> Epoch: 12 Training Average loss: 1.0146, Validating Average loss: 1.0314\n",
      "====> Epoch: 13 Training Average loss: 1.0101, Validating Average loss: 1.0271\n",
      "====> Epoch: 14 Training Average loss: 1.0062, Validating Average loss: 1.0212\n",
      "====> Epoch: 15 Training Average loss: 1.0035, Validating Average loss: 1.0235\n",
      "====> Epoch: 16 Training Average loss: 1.0010, Validating Average loss: 1.0212\n",
      "====> Epoch: 17 Training Average loss: 0.9997, Validating Average loss: 1.0179\n",
      "====> Epoch: 18 Training Average loss: 0.9976, Validating Average loss: 1.0205\n",
      "====> Epoch: 19 Training Average loss: 0.9963, Validating Average loss: 1.0203\n",
      "====> Epoch: 20 Training Average loss: 0.9955, Validating Average loss: 1.0185\n",
      "====> Epoch: 21 Training Average loss: 0.9945, Validating Average loss: 1.0180\n",
      "====> Epoch: 22 Training Average loss: 0.9946, Validating Average loss: 1.0152\n",
      "====> Epoch: 23 Training Average loss: 0.9934, Validating Average loss: 1.0156\n",
      "====> Epoch: 24 Training Average loss: 0.9930, Validating Average loss: 1.0157\n",
      "====> Epoch: 25 Training Average loss: 0.9924, Validating Average loss: 1.0159\n",
      "====> Epoch: 26 Training Average loss: 0.9915, Validating Average loss: 1.0148\n",
      "====> Epoch: 27 Training Average loss: 0.9920, Validating Average loss: 1.0166\n",
      "====> Epoch: 28 Training Average loss: 0.9913, Validating Average loss: 1.0172\n",
      "====> Epoch: 29 Training Average loss: 0.9910, Validating Average loss: 1.0157\n",
      "====> Epoch: 30 Training Average loss: 0.9900, Validating Average loss: 1.0137\n",
      "====> Epoch: 31 Training Average loss: 0.9899, Validating Average loss: 1.0131\n",
      "====> Epoch: 32 Training Average loss: 0.9896, Validating Average loss: 1.0108\n",
      "====> Epoch: 33 Training Average loss: 0.9901, Validating Average loss: 1.0151\n",
      "====> Epoch: 34 Training Average loss: 0.9893, Validating Average loss: 1.0146\n",
      "====> Epoch: 35 Training Average loss: 0.9886, Validating Average loss: 1.0128\n",
      "====> Epoch: 36 Training Average loss: 0.9886, Validating Average loss: 1.0150\n",
      "====> Epoch: 37 Training Average loss: 0.9882, Validating Average loss: 1.0141\n",
      "====> Epoch: 38 Training Average loss: 0.9888, Validating Average loss: 1.0119\n",
      "====> Epoch: 39 Training Average loss: 0.9880, Validating Average loss: 1.0132\n",
      "====> Epoch: 40 Training Average loss: 0.9875, Validating Average loss: 1.0126\n",
      "====> Epoch: 41 Training Average loss: 0.9876, Validating Average loss: 1.0155\n",
      "====> Epoch: 42 Training Average loss: 0.9878, Validating Average loss: 1.0129\n",
      "====> Epoch: 43 Training Average loss: 0.9875, Validating Average loss: 1.0120\n",
      "====> Epoch: 44 Training Average loss: 0.9875, Validating Average loss: 1.0128\n",
      "====> Epoch: 45 Training Average loss: 0.9870, Validating Average loss: 1.0137\n",
      "====> Epoch: 46 Training Average loss: 0.9871, Validating Average loss: 1.0101\n",
      "====> Epoch: 47 Training Average loss: 0.9870, Validating Average loss: 1.0111\n",
      "====> Epoch: 48 Training Average loss: 0.9878, Validating Average loss: 1.0127\n",
      "====> Epoch: 49 Training Average loss: 0.9865, Validating Average loss: 1.0132\n",
      "====> Epoch: 50 Training Average loss: 0.9862, Validating Average loss: 1.0118\n",
      "====> Epoch: 51 Training Average loss: 0.9860, Validating Average loss: 1.0082\n",
      "====> Epoch: 52 Training Average loss: 0.9859, Validating Average loss: 1.0105\n",
      "====> Epoch: 53 Training Average loss: 0.9861, Validating Average loss: 1.0127\n",
      "====> Epoch: 54 Training Average loss: 0.9858, Validating Average loss: 1.0101\n",
      "====> Epoch: 55 Training Average loss: 0.9860, Validating Average loss: 1.0113\n",
      "====> Epoch: 56 Training Average loss: 0.9855, Validating Average loss: 1.0141\n",
      "====> Epoch: 57 Training Average loss: 0.9859, Validating Average loss: 1.0139\n",
      "====> Epoch: 58 Training Average loss: 0.9859, Validating Average loss: 1.0077\n",
      "====> Epoch: 59 Training Average loss: 0.9849, Validating Average loss: 1.0085\n",
      "====> Epoch: 60 Training Average loss: 0.9856, Validating Average loss: 1.0110\n",
      "====> Epoch: 61 Training Average loss: 0.9849, Validating Average loss: 1.0098\n",
      "====> Epoch: 62 Training Average loss: 0.9850, Validating Average loss: 1.0141\n",
      "====> Epoch: 63 Training Average loss: 0.9851, Validating Average loss: 1.0093\n",
      "====> Epoch: 64 Training Average loss: 0.9845, Validating Average loss: 1.0114\n",
      "====> Epoch: 65 Training Average loss: 0.9845, Validating Average loss: 1.0101\n",
      "====> Epoch: 66 Training Average loss: 0.9843, Validating Average loss: 1.0121\n",
      "====> Epoch: 67 Training Average loss: 0.9839, Validating Average loss: 1.0103\n",
      "====> Epoch: 68 Training Average loss: 0.9845, Validating Average loss: 1.0089\n",
      "====> Epoch: 69 Training Average loss: 0.9836, Validating Average loss: 1.0086\n",
      "====> Epoch: 70 Training Average loss: 0.9844, Validating Average loss: 1.0069\n",
      "====> Epoch: 71 Training Average loss: 0.9843, Validating Average loss: 1.0076\n",
      "====> Epoch: 72 Training Average loss: 0.9838, Validating Average loss: 1.0059\n",
      "====> Epoch: 73 Training Average loss: 0.9834, Validating Average loss: 1.0089\n",
      "====> Epoch: 74 Training Average loss: 0.9840, Validating Average loss: 1.0125\n",
      "====> Epoch: 75 Training Average loss: 0.9835, Validating Average loss: 1.0102\n",
      "====> Epoch: 76 Training Average loss: 0.9833, Validating Average loss: 1.0115\n",
      "====> Epoch: 77 Training Average loss: 0.9834, Validating Average loss: 1.0063\n",
      "====> Epoch: 78 Training Average loss: 0.9829, Validating Average loss: 1.0129\n",
      "====> Epoch: 79 Training Average loss: 0.9829, Validating Average loss: 1.0080\n",
      "====> Epoch: 80 Training Average loss: 0.9831, Validating Average loss: 1.0067\n",
      "====> Epoch: 81 Training Average loss: 0.9833, Validating Average loss: 1.0061\n",
      "====> Epoch: 82 Training Average loss: 0.9830, Validating Average loss: 1.0106\n",
      "====> Epoch: 83 Training Average loss: 0.9827, Validating Average loss: 1.0108\n",
      "====> Epoch: 84 Training Average loss: 0.9828, Validating Average loss: 1.0085\n",
      "====> Epoch: 85 Training Average loss: 0.9823, Validating Average loss: 1.0116\n",
      "====> Epoch: 86 Training Average loss: 0.9825, Validating Average loss: 1.0103\n",
      "====> Epoch: 87 Training Average loss: 0.9831, Validating Average loss: 1.0089\n",
      "====> Epoch: 88 Training Average loss: 0.9825, Validating Average loss: 1.0116\n",
      "====> Epoch: 89 Training Average loss: 0.9822, Validating Average loss: 1.0091\n",
      "====> Epoch: 90 Training Average loss: 0.9821, Validating Average loss: 1.0088\n",
      "====> Epoch: 91 Training Average loss: 0.9822, Validating Average loss: 1.0066\n",
      "====> Epoch: 92 Training Average loss: 0.9826, Validating Average loss: 1.0103\n",
      "====> Epoch: 93 Training Average loss: 0.9820, Validating Average loss: 1.0081\n",
      "====> Epoch: 94 Training Average loss: 0.9815, Validating Average loss: 1.0082\n",
      "====> Epoch: 95 Training Average loss: 0.9823, Validating Average loss: 1.0072\n",
      "====> Epoch: 96 Training Average loss: 0.9821, Validating Average loss: 1.0087\n",
      "====> Epoch: 97 Training Average loss: 0.9816, Validating Average loss: 1.0085\n",
      "====> Epoch: 98 Training Average loss: 0.9818, Validating Average loss: 1.0078\n",
      "====> Epoch: 99 Training Average loss: 0.9824, Validating Average loss: 1.0093\n",
      "====> Epoch: 100 Training Average loss: 0.9815, Validating Average loss: 1.0064\n",
      "====> Epoch: 101 Training Average loss: 0.9811, Validating Average loss: 1.0077\n",
      "====> Epoch: 102 Training Average loss: 0.9814, Validating Average loss: 1.0071\n",
      "====> Epoch: 103 Training Average loss: 0.9817, Validating Average loss: 1.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 104 Training Average loss: 0.9810, Validating Average loss: 1.0054\n",
      "====> Epoch: 105 Training Average loss: 0.9807, Validating Average loss: 1.0072\n",
      "====> Epoch: 106 Training Average loss: 0.9810, Validating Average loss: 1.0056\n",
      "====> Epoch: 107 Training Average loss: 0.9813, Validating Average loss: 1.0074\n",
      "====> Epoch: 108 Training Average loss: 0.9809, Validating Average loss: 1.0083\n",
      "====> Epoch: 109 Training Average loss: 0.9806, Validating Average loss: 1.0063\n",
      "====> Epoch: 110 Training Average loss: 0.9807, Validating Average loss: 1.0081\n",
      "====> Epoch: 111 Training Average loss: 0.9809, Validating Average loss: 1.0082\n",
      "====> Epoch: 112 Training Average loss: 0.9802, Validating Average loss: 1.0084\n",
      "====> Epoch: 113 Training Average loss: 0.9799, Validating Average loss: 1.0081\n",
      "====> Epoch: 114 Training Average loss: 0.9806, Validating Average loss: 1.0086\n",
      "====> Epoch: 115 Training Average loss: 0.9802, Validating Average loss: 1.0058\n",
      "====> Epoch: 116 Training Average loss: 0.9800, Validating Average loss: 1.0102\n",
      "====> Epoch: 117 Training Average loss: 0.9807, Validating Average loss: 1.0057\n",
      "====> Epoch: 118 Training Average loss: 0.9801, Validating Average loss: 1.0102\n",
      "====> Epoch: 119 Training Average loss: 0.9796, Validating Average loss: 1.0080\n",
      "====> Epoch: 120 Training Average loss: 0.9799, Validating Average loss: 1.0067\n",
      "====> Epoch: 121 Training Average loss: 0.9799, Validating Average loss: 1.0057\n",
      "====> Epoch: 122 Training Average loss: 0.9802, Validating Average loss: 1.0081\n",
      "====> Epoch: 123 Training Average loss: 0.9800, Validating Average loss: 1.0074\n",
      "====> Epoch: 124 Training Average loss: 0.9796, Validating Average loss: 1.0055\n",
      "====> Epoch: 125 Training Average loss: 0.9801, Validating Average loss: 1.0059\n",
      "====> Epoch: 126 Training Average loss: 0.9803, Validating Average loss: 1.0083\n",
      "====> Epoch: 127 Training Average loss: 0.9802, Validating Average loss: 1.0069\n",
      "====> Epoch: 128 Training Average loss: 0.9798, Validating Average loss: 1.0057\n",
      "====> Epoch: 129 Training Average loss: 0.9798, Validating Average loss: 1.0096\n",
      "====> Epoch: 130 Training Average loss: 0.9793, Validating Average loss: 1.0072\n",
      "====> Epoch: 131 Training Average loss: 0.9796, Validating Average loss: 1.0068\n",
      "====> Epoch: 132 Training Average loss: 0.9796, Validating Average loss: 1.0078\n",
      "====> Epoch: 133 Training Average loss: 0.9795, Validating Average loss: 1.0068\n",
      "====> Epoch: 134 Training Average loss: 0.9796, Validating Average loss: 1.0071\n",
      "====> Epoch: 135 Training Average loss: 0.9793, Validating Average loss: 1.0056\n",
      "====> Epoch: 136 Training Average loss: 0.9795, Validating Average loss: 1.0067\n",
      "====> Epoch: 137 Training Average loss: 0.9793, Validating Average loss: 1.0104\n",
      "====> Epoch: 138 Training Average loss: 0.9788, Validating Average loss: 1.0065\n",
      "====> Epoch: 139 Training Average loss: 0.9787, Validating Average loss: 1.0073\n",
      "====> Epoch: 140 Training Average loss: 0.9792, Validating Average loss: 1.0065\n",
      "====> Epoch: 141 Training Average loss: 0.9788, Validating Average loss: 1.0043\n",
      "====> Epoch: 142 Training Average loss: 0.9791, Validating Average loss: 1.0066\n",
      "====> Epoch: 143 Training Average loss: 0.9792, Validating Average loss: 1.0077\n",
      "====> Epoch: 144 Training Average loss: 0.9790, Validating Average loss: 1.0084\n",
      "====> Epoch: 145 Training Average loss: 0.9790, Validating Average loss: 1.0048\n",
      "====> Epoch: 146 Training Average loss: 0.9782, Validating Average loss: 1.0104\n",
      "====> Epoch: 147 Training Average loss: 0.9790, Validating Average loss: 1.0090\n",
      "====> Epoch: 148 Training Average loss: 0.9783, Validating Average loss: 1.0056\n",
      "====> Epoch: 149 Training Average loss: 0.9786, Validating Average loss: 1.0053\n",
      "====> Epoch: 150 Training Average loss: 0.9785, Validating Average loss: 1.0077\n",
      "====> Epoch: 151 Training Average loss: 0.9785, Validating Average loss: 1.0067\n",
      "====> Epoch: 152 Training Average loss: 0.9783, Validating Average loss: 1.0081\n",
      "====> Epoch: 153 Training Average loss: 0.9778, Validating Average loss: 1.0078\n",
      "====> Epoch: 154 Training Average loss: 0.9793, Validating Average loss: 1.0062\n",
      "====> Epoch: 155 Training Average loss: 0.9785, Validating Average loss: 1.0055\n",
      "====> Epoch: 156 Training Average loss: 0.9783, Validating Average loss: 1.0105\n",
      "====> Epoch: 157 Training Average loss: 0.9786, Validating Average loss: 1.0042\n",
      "====> Epoch: 158 Training Average loss: 0.9786, Validating Average loss: 1.0060\n",
      "====> Epoch: 159 Training Average loss: 0.9782, Validating Average loss: 1.0074\n",
      "====> Epoch: 160 Training Average loss: 0.9788, Validating Average loss: 1.0051\n",
      "====> Epoch: 161 Training Average loss: 0.9779, Validating Average loss: 1.0062\n",
      "====> Epoch: 162 Training Average loss: 0.9775, Validating Average loss: 1.0074\n",
      "====> Epoch: 163 Training Average loss: 0.9780, Validating Average loss: 1.0048\n",
      "====> Epoch: 164 Training Average loss: 0.9780, Validating Average loss: 1.0057\n",
      "====> Epoch: 165 Training Average loss: 0.9779, Validating Average loss: 1.0058\n",
      "====> Epoch: 166 Training Average loss: 0.9784, Validating Average loss: 1.0053\n",
      "====> Epoch: 167 Training Average loss: 0.9779, Validating Average loss: 1.0047\n",
      "====> Epoch: 168 Training Average loss: 0.9778, Validating Average loss: 1.0056\n",
      "====> Epoch: 169 Training Average loss: 0.9777, Validating Average loss: 1.0052\n",
      "====> Epoch: 170 Training Average loss: 0.9782, Validating Average loss: 1.0073\n",
      "====> Epoch: 171 Training Average loss: 0.9780, Validating Average loss: 1.0088\n",
      "====> Epoch: 172 Training Average loss: 0.9781, Validating Average loss: 1.0053\n",
      "====> Epoch: 173 Training Average loss: 0.9776, Validating Average loss: 1.0055\n",
      "====> Epoch: 174 Training Average loss: 0.9773, Validating Average loss: 1.0052\n",
      "====> Epoch: 175 Training Average loss: 0.9778, Validating Average loss: 1.0034\n",
      "====> Epoch: 176 Training Average loss: 0.9778, Validating Average loss: 1.0055\n",
      "====> Epoch: 177 Training Average loss: 0.9775, Validating Average loss: 1.0073\n",
      "====> Epoch: 178 Training Average loss: 0.9778, Validating Average loss: 1.0055\n",
      "====> Epoch: 179 Training Average loss: 0.9778, Validating Average loss: 1.0057\n",
      "====> Epoch: 180 Training Average loss: 0.9782, Validating Average loss: 1.0054\n",
      "====> Epoch: 181 Training Average loss: 0.9778, Validating Average loss: 1.0060\n",
      "====> Epoch: 182 Training Average loss: 0.9774, Validating Average loss: 1.0067\n",
      "====> Epoch: 183 Training Average loss: 0.9772, Validating Average loss: 1.0063\n",
      "====> Epoch: 184 Training Average loss: 0.9771, Validating Average loss: 1.0055\n",
      "====> Epoch: 185 Training Average loss: 0.9777, Validating Average loss: 1.0053\n",
      "====> Epoch: 186 Training Average loss: 0.9775, Validating Average loss: 1.0052\n",
      "====> Epoch: 187 Training Average loss: 0.9774, Validating Average loss: 1.0037\n",
      "====> Epoch: 188 Training Average loss: 0.9773, Validating Average loss: 1.0083\n",
      "====> Epoch: 189 Training Average loss: 0.9771, Validating Average loss: 1.0048\n",
      "====> Epoch: 190 Training Average loss: 0.9773, Validating Average loss: 1.0052\n",
      "====> Epoch: 191 Training Average loss: 0.9781, Validating Average loss: 1.0058\n",
      "====> Epoch: 192 Training Average loss: 0.9773, Validating Average loss: 1.0047\n",
      "====> Epoch: 193 Training Average loss: 0.9773, Validating Average loss: 1.0064\n",
      "====> Epoch: 194 Training Average loss: 0.9768, Validating Average loss: 1.0062\n",
      "====> Epoch: 195 Training Average loss: 0.9768, Validating Average loss: 1.0056\n",
      "====> Epoch: 196 Training Average loss: 0.9769, Validating Average loss: 1.0047\n",
      "====> Epoch: 197 Training Average loss: 0.9773, Validating Average loss: 1.0072\n",
      "====> Epoch: 198 Training Average loss: 0.9766, Validating Average loss: 1.0059\n",
      "====> Epoch: 199 Training Average loss: 0.9769, Validating Average loss: 1.0058\n",
      "====> Epoch: 200 Training Average loss: 0.9767, Validating Average loss: 1.0035\n",
      "\n",
      " ==================== Testing Variational Auto Encoder on device: cuda:0 ====================\n",
      "Average test loss: 0.9789\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, dev_loader=None, is_validate=True, device=\"cuda:0\"):\n",
    "  vae.train()\n",
    "  total_loss = 0\n",
    "  for _, data in enumerate(train_loader, 0):\n",
    "    data = Variable(data[0]).to(device)\n",
    "    target = data.clone()\n",
    "    optimizer.zero_grad()\n",
    "    recon_x, mu, logvar = vae.forward(data)\n",
    "    recon_x[target == 0] = 0\n",
    "    loss = loss_func(recon_x, data, mu, logvar)\n",
    "    loss.backward()\n",
    "    total_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    epoch_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "  if (is_validate == True):\n",
    "    vae.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for _, data in enumerate(dev_loader, 0):\n",
    "        data = Variable(data[0]).to(device)\n",
    "        target = data.clone()\n",
    "        recon_x, mu, logvar = vae.forward(data)\n",
    "        recon_x[target == 0] = 0\n",
    "        loss = loss_func(recon_x, data, mu, logvar)\n",
    "        total_loss += loss.item()\n",
    "        epoch_dev_loss = total_loss / len(dev_loader)\n",
    "    print('====> Epoch: {} Training Average loss: {:.4f}, Validating Average loss: {:.4f}'.format(epoch, epoch_train_loss, epoch_dev_loss))\n",
    "    return epoch_train_loss, epoch_dev_loss\n",
    "\n",
    "  else:\n",
    "    print('====> Epoch: {} Training Average loss: {:.4f}'.format(epoch, epoch_train_loss))\n",
    "    return epoch_train_loss\n",
    "\n",
    "def test(test_loader, device=\"cuda:0\"):\n",
    "  vae.eval()\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for _, data in enumerate(test_loader, 0):\n",
    "      data = Variable(data[0]).to(device)\n",
    "      target = data.clone()\n",
    "      recon_x, mu, logvar = vae.forward(data)\n",
    "      recon_x[target == 0] = 0\n",
    "      loss = loss_func(recon_x, data, mu, logvar)\n",
    "      total_loss += loss.item()\n",
    "  print('Average test loss: {:.4f}'.format(total_loss / len(test_loader)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  vae = VAE(nb_movies = nb_movies).to(device)\n",
    "  optimizer =  optim.Adam(vae.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "  EPOCH = 200\n",
    "  epoches_train_loss = []\n",
    "  epoches_dev_loss = []\n",
    "\n",
    "  print(\"\\n\", 20 * \"=\", \"Training Variational Auto Encoder on device: {}\".format(device), 20 * \"=\")\n",
    "  for epoch in range(1, EPOCH + 1):\n",
    "    epoch_train_loss, epoch_dev_loss = train(train_loader, dev_loader = dev_loader)\n",
    "    epoches_train_loss.append(epoch_train_loss)\n",
    "    epoches_dev_loss.append(epoch_dev_loss)\n",
    "  print(\"\\n\", 20 * \"=\", \"Testing Variational Auto Encoder on device: {}\".format(device), 20 * \"=\")\n",
    "  test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whP5w_Lc_qWw"
   },
   "source": [
    "### 3.3 Plot a graph to show the loss change in the training and verification process. It is found that although the training loss of VAE is larger than AE, the validating loss and test loss are smaller than AE. The consideration is because the training loss of VAE contains two parts, so the training loss is too large; but the VAE model may capture more complex patterns in the data, making it perform better on the validation set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1605471495077,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "Stk1vrbEfkM1",
    "outputId": "1b626262-7a5b-4521-8630-0bdc5e384b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99f9af8470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(np.arange(len(epoches_train_loss)), epoches_train_loss, color='r', label='training loss')\n",
    "plt.plot(np.arange(len(epoches_dev_loss)), epoches_dev_loss, color='b', label='validating loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('MSE-loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRDJM3ECArls"
   },
   "source": [
    "## **4. Use the model to predict the movies that the user has not rated, and according to the predicted scores, recommend movies that the user has not watched**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T36nVcojeiEv"
   },
   "source": [
    "### 4.1 Define the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1356,
     "status": "ok",
     "timestamp": 1605472072694,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "fDzzf4JYAyQX"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "def Prediction(model, movies, user_id, nb_recommend):\n",
    "    user_input = Variable(test_set[user_id - 1]).unsqueeze(0).to(device)\n",
    "    predict_output, _, _ = model.forward(user_input)\n",
    "    predict_output = predict_output.cpu().detach().numpy()\n",
    "    user_input = user_input.cpu().detach().numpy()\n",
    "    predicted_result = np.vstack([user_input, predict_output])\n",
    "    recommend = np.array(predicted_result)\n",
    "    trian_movie_id = np.array([i for i in range(1, nb_movies+1)]) #create a temporary index for movies since we are going to delete some movies that the user had seen\n",
    "    recommend = np.row_stack((recommend, trian_movie_id)) #insert that index into the result array, \n",
    "    recommend = recommend.T #transpose row and col Invert the rows and columns of the array\n",
    "    recommend = recommend.tolist() #tansfer into list for further process\n",
    "\n",
    "    movie_not_seen = [] #delete the rows comtaining the movies that the user had seen\n",
    "    for i in range(len(recommend)):\n",
    "        if recommend[i][0] == 0.0:\n",
    "            movie_not_seen.append(recommend[i])\n",
    "\n",
    "    movie_not_seen = sorted(movie_not_seen, key=itemgetter(1), reverse=True) #sort the movies by mark Sort the movies in descending order according to the predicted score\n",
    "\n",
    "    recommend_movie = [] #create list for recommended movies with the index we created recommended top20\n",
    "    for i in range(0, nb_recommend):\n",
    "        recommend_movie.append(movie_not_seen[i][2])\n",
    "\n",
    "    recommend_index = [] #get the real index in the original file of'movies.dat' by using the temporary index\n",
    "    for i in range(len(recommend_movie)):\n",
    "        recommend_index.append(movies[(movies.iloc[:,0]==recommend_movie[i])].index.tolist())\n",
    "\n",
    "    recommend_movie_name = [] #get a list of movie names using the real indexEnter the corresponding index and export the movie names\n",
    "    for i in range(len(recommend_index)):\n",
    "        np_movie = movies.iloc[recommend_index[i],1].values #transefer to np.array\n",
    "        list_movie = np_movie.tolist() #transfer to list\n",
    "        recommend_movie_name.append(list_movie)\n",
    "\n",
    "    print('Highly Recommended Moives for You:\\n')\n",
    "    for i in range(len(recommend_movie_name)):\n",
    "        print(str(recommend_movie_name[i]))\n",
    "    \n",
    "    return recommend_movie_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snJRCgx3emKd"
   },
   "source": [
    "### 4.2 To the designated user, recommend his most likely favorite movies from the movies he hasnâ€™t watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1605472083401,
     "user": {
      "displayName": "Yuxin JIANG",
      "photoUrl": "",
      "userId": "08237551511886881536"
     },
     "user_tz": -480
    },
    "id": "LMC_L9axA4VQ",
    "outputId": "ce7b5bfc-95d8-46a1-9e3c-e19a6c374a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Recommended Moives for You:\n",
      "\n",
      "['CallejÃ³n de los milagros, El (1995)']\n",
      "['Lured (1947)']\n",
      "['Modulations (1998)']\n",
      "['Return with Honor (1998)']\n",
      "['Song of Freedom (1936)']\n",
      "['Lamerica (1994)']\n",
      "['Follow the Bitch (1998)']\n",
      "['Bittersweet Motel (2000)']\n",
      "['One Little Indian (1973)']\n",
      "['Skipped Parts (2000)']\n",
      "['Eighth Day, The (Le HuitiÃ¨me jour ) (1996)']\n",
      "['Zachariah (1971)']\n",
      "['Smashing Time (1967)']\n",
      "['Window to Paris (1994)']\n",
      "['Hour of the Pig, The (1993)']\n",
      "['Ulysses (Ulisse) (1954)']\n",
      "['Soft Fruit (1999)']\n",
      "['Firelight (1997)']\n",
      "['Gate of Heavenly Peace, The (1995)']\n",
      "['Sanjuro (1962)']\n"
     ]
    }
   ],
   "source": [
    "# Specify the user's ID\n",
    "user_id = 86\n",
    "# Specify the number of recommended movies\n",
    "nb_recommend = 20\n",
    "# Read the original movie data for query when recommending\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "\n",
    "# Use VAE model for prediction recommendation\n",
    "movie_for_you = Prediction(model = vae, movies = movies, user_id = user_id, nb_recommend = nb_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOVIFrFbtBcOk936NgR/ySu",
   "collapsed_sections": [],
   "name": "auto encoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
